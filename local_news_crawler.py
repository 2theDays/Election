"""
ì¶©ë¶ ì§€ì—­ ì‹ ë¬¸ì‚¬ RSS ê¸°ë°˜ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°
ë„¤ì´ë²„ ì°¨ë‹¨ ìš°íšŒ - ì§€ì—­ ì–¸ë¡ ì‚¬ + êµ¬ê¸€ ì•Œë¦¼ í™œìš©
"""

import feedparser
import requests
from bs4 import BeautifulSoup
import json
import pandas as pd
from datetime import datetime
import time
import google.generativeai as genai
import os
import re

# .env ë¡œë“œ í•¨ìˆ˜
def load_env():
    try:
        with open('.env', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'): continue
                if '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key] = value.strip()
    except: pass

load_env()

# Gemini API ì„¤ì •
api_key = os.environ.get("GEMINI_API_KEY")
if api_key:
    genai.configure(api_key=api_key)

# ============================================================================
# 1. ì¶©ë¶ ì§€ì—­ ì‹ ë¬¸ì‚¬ RSS ì£¼ì†Œ ëª©ë¡
# ============================================================================

REGIONAL_NEWS_SOURCES = {
    # === ì£¼ìš” ì¼ê°„ì§€ ===
    "ì¶©ë¶ì¼ë³´": {
        "rss": [
            "https://www.inews365.com/rss/allArticle.xml",
            "https://www.inews365.com/rss/S1N1.xml",  # ì •ì¹˜
            "https://www.inews365.com/news/rss.xml",
        ],
        "search_url": "https://www.inews365.com",
        "type": "ì¼ê°„ì§€"
    },
    "ì¶©ì²­ì¼ë³´": {
        "rss": [
            "https://www.ccdailynews.com/rss/allArticle.xml",
            "https://www.ccdailynews.com/rss/S1N1.xml",
        ],
        "search_url": "https://www.ccdailynews.com",
        "type": "ì¼ê°„ì§€"
    },
    "ì¤‘ë¶€ë§¤ì¼": {
        "rss": [
            "https://www.jbnews.com/rss/allArticle.xml",
            "https://www.jbnews.com/rss/S1N1.xml",
        ],
        "search_url": "https://www.jbnews.com",
        "type": "ì¼ê°„ì§€"
    },
    "ë™ì–‘ì¼ë³´": {
        "rss": [
            "http://www.dynews.co.kr/rss/allArticle.xml",
            "http://www.dynews.co.kr/news/rss.xml",
        ],
        "search_url": "http://www.dynews.co.kr",
        "type": "ì¼ê°„ì§€"
    },
    "ì¶©ì²­íƒ€ì„ì¦ˆ": {
        "rss": [
            "http://www.cctimes.kr/rss/allArticle.xml",
            "http://www.cctimes.kr/news/rss.xml",
        ],
        "search_url": "http://www.cctimes.kr",
        "type": "ì¼ê°„ì§€"
    },
    "ì¶©ì²­íˆ¬ë°ì´": {
        "rss": [
            "http://www.cctoday.co.kr/rss/allArticle.xml",
            "http://www.cctoday.co.kr/news/rss.xml",
        ],
        "search_url": "http://www.cctoday.co.kr",
        "type": "ì¼ê°„ì§€"
    },
    
    # === ì¸í„°ë„· ì‹ ë¬¸ ===
    "êµ¿ëª¨ë‹ì¶©ì²­": {
        "rss": [
            "https://www.goodmorningcc.com/rss/allArticle.xml",
            "https://www.goodmorningcc.com/news/rss.xml",
        ],
        "search_url": "https://www.goodmorningcc.com",
        "type": "ì¸í„°ë„·ì‹ ë¬¸"
    },
    "ì¶©ë¶ì¸ë‰´ìŠ¤": {
        "rss": [
            "https://www.cbinews.co.kr/rss/allArticle.xml",
        ],
        "search_url": "https://www.cbinews.co.kr",
        "type": "ì¸í„°ë„·ì‹ ë¬¸"
    },
    
    # === ì£¼ê°„ì§€ ===
    "ì˜¥ì²œì‹ ë¬¸": {
        "rss": [
            "http://www.okinews.com/rss/allArticle.xml",
        ],
        "search_url": "http://www.okinews.com",
        "type": "ì£¼ê°„ì§€"
    },
}

# êµ¬ê¸€ ì•Œë¦¼ RSS (ì¶©ë¶ ë„ì§€ì‚¬ ê´€ë ¨ í‚¤ì›Œë“œ)
# ì‚¬ìš©ìê°€ ì§ì ‘ ë§Œë“¤ì–´ì•¼ í•¨: https://www.google.com/alerts
GOOGLE_ALERTS_RSS = [
    # ì˜ˆì‹œ - ì‹¤ì œ RSS ì£¼ì†Œë¡œ êµì²´í•˜ì„¸ìš”:
    # "https://www.google.com/alerts/feeds/12345678901234567890/ì‹ ìš©í•œì¶©ë¶ë„ì§€ì‚¬",
    # "https://www.google.com/alerts/feeds/12345678901234567890/ë…¸ì˜ë¯¼ì¶©ë¶ë„ì§€ì‚¬",
    # "https://www.google.com/alerts/feeds/12345678901234567890/ì†¡ê¸°ì„­ì¶©ë¶ë„ì§€ì‚¬",
    # "https://www.google.com/alerts/feeds/12345678901234567890/í•œë²”ë•ì¶©ë¶ë„ì§€ì‚¬",
    # "https://www.google.com/alerts/feeds/12345678901234567890/ìœ¤í¬ê·¼ì¶©ë¶ë„ì§€ì‚¬",
    # "https://www.google.com/alerts/feeds/12345678901234567890/ì´ì¢…ë°°ì¶©ë¶ë„ì§€ì‚¬",
    # "https://www.google.com/alerts/feeds/12345678901234567890/ì¶©ë¶ë„ì§€ì‚¬ì„ ê±°",
    # "https://www.google.com/alerts/feeds/12345678901234567890/2026ì¶©ë¶ë„ì§€ì‚¬",
]

# ============================================================================
# 2. RSS í”¼ë“œ íŒŒì„œ
# ============================================================================

class LocalNewsCollector:
    """ì§€ì—­ ì‹ ë¬¸ì‚¬ RSS ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.articles = []
        # í›„ë³´ì ë°ì´í„° ë¡œë“œ
        with open('candidates_data.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        self.candidates = [c['name'] for c in data['candidates']]
        self.working_rss_urls = []  # ì‘ë™í•˜ëŠ” RSS ì£¼ì†Œ ì €ì¥
    
    def test_rss_url(self, url):
        """RSS URLì´ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
        try:
            feed = feedparser.parse(url)
            if feed.entries and len(feed.entries) > 0:
                return True
            return False
        except:
            return False
    
    def find_working_rss(self, base_url, source_name):
        """ì—¬ëŸ¬ RSS íŒ¨í„´ì„ ì‹œë„í•´ì„œ ì‘ë™í•˜ëŠ” ê²ƒ ì°¾ê¸°"""
        patterns = [
            "/rss/allArticle.xml",
            "/rss/S1N1.xml",
            "/news/rss.xml",
            "/rss.xml",
            "/feed",
            "/rss",
            "/rss/news.xml",
        ]
        
        working_urls = []
        
        for pattern in patterns:
            test_url = base_url.rstrip('/') + pattern
            if self.test_rss_url(test_url):
                working_urls.append(test_url)
                print(f"    âœ… ë°œê²¬: {test_url}")
        
        return working_urls
    
    def collect_from_rss(self, rss_url, source_name):
        """RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        try:
            print(f"  ì‹œë„ ì¤‘: {rss_url}")
            
            # RSS íŒŒì‹±
            feed = feedparser.parse(rss_url)
            
            if not feed.entries:
                print(f"    âš ï¸  í”¼ë“œê°€ ë¹„ì–´ìˆìŒ ë˜ëŠ” ì£¼ì†Œ ì˜¤ë¥˜")
                return []
            
            # ì‘ë™í•˜ëŠ” URL ì €ì¥
            if rss_url not in self.working_rss_urls:
                self.working_rss_urls.append(rss_url)
            
            articles_found = []
            
            for entry in feed.entries:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = entry.get('title', '')
                link = entry.get('link', '')
                summary = entry.get('summary', entry.get('description', ''))
                published = entry.get('published', entry.get('updated', ''))
                
                # í›„ë³´ì ì´ë¦„ì´ í¬í•¨ëœ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘
                full_text = title + " " + summary
                if any(name in full_text for name in self.candidates):
                    articles_found.append({
                        'title': title,
                        'content': BeautifulSoup(summary, 'html.parser').get_text(),
                        'url': link,
                        'date': published,
                        'source': source_name,
                        'keyword': 'í›„ë³´ìëª…'
                    })
            
            print(f"    âœ… {len(articles_found)}ê°œ ê´€ë ¨ ê¸°ì‚¬ ë°œê²¬ (ì „ì²´ {len(feed.entries)}ê°œ)")
            return articles_found
            
        except Exception as e:
            print(f"    âŒ ì˜¤ë¥˜: {e}")
            return []
    
    def collect_all(self):
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘"""
        print("\n" + "="*60)
        print("ì¶©ë¶ ì§€ì—­ ì–¸ë¡ ì‚¬ RSS ìˆ˜ì§‘ ì‹œì‘")
        print("="*60 + "\n")
        
        all_articles = []
        
        # 1. ì§€ì—­ ì‹ ë¬¸ì‚¬ RSS
        for source_name, source_info in REGIONAL_NEWS_SOURCES.items():
            print(f"\nã€{source_name}ã€‘ ({source_info.get('type', 'ì–¸ë¡ ì‚¬')})")
            
            # ë¨¼ì € ì œê³µëœ RSS ì£¼ì†Œ ì‹œë„
            found_articles = False
            for rss_url in source_info['rss']:
                articles = self.collect_from_rss(rss_url, source_name)
                if articles:
                    all_articles.extend(articles)
                    found_articles = True
                time.sleep(0.5)
            
            # RSSê°€ ì•ˆ ë˜ë©´ ìë™ íƒì§€ ì‹œë„
            if not found_articles:
                print(f"  ğŸ’¡ ìë™ íƒì§€ ëª¨ë“œ...")
                working_urls = self.find_working_rss(
                    source_info['search_url'], 
                    source_name
                )
                
                for url in working_urls:
                    articles = self.collect_from_rss(url, source_name)
                    all_articles.extend(articles)
                    time.sleep(0.5)
        
        # 2. êµ¬ê¸€ ì•Œë¦¼ RSS (ìˆëŠ” ê²½ìš°)
        if GOOGLE_ALERTS_RSS:
            print(f"\nã€êµ¬ê¸€ ì•Œë¦¼ã€‘")
            for rss_url in GOOGLE_ALERTS_RSS:
                if rss_url.startswith("http"):  # ì£¼ì„ì´ ì•„ë‹Œ ì‹¤ì œ URLë§Œ
                    articles = self.collect_from_rss(rss_url, "êµ¬ê¸€ì•Œë¦¼")
                    all_articles.extend(articles)
                    time.sleep(0.5)
        
        # 3. ì‘ë™í•˜ëŠ” RSS ì£¼ì†Œ ì¶œë ¥
        if self.working_rss_urls:
            print(f"\n" + "="*60)
            print("âœ… ì‘ë™ í™•ì¸ëœ RSS ì£¼ì†Œ")
            print("="*60)
            for url in self.working_rss_urls:
                print(f"  {url}")
            print("\nğŸ’¡ ë‹¤ìŒë²ˆì—” ì´ ì£¼ì†Œë“¤ë§Œ ì‚¬ìš©í•˜ë©´ ë” ë¹ ë¦…ë‹ˆë‹¤!")
        
        return all_articles
    
    def extract_relationships_with_claude(self, article):
        """Claude APIë¡œ ê´€ê³„ ì¶”ì¶œ"""
        mentioned_candidates = [c for c in self.candidates 
                               if c in article['title'] + article['content']]
        
        if len(mentioned_candidates) < 1:
            return []
        
        prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ ì¶©ì²­ë¶ë„ ë„ì§€ì‚¬ í›„ë³´ìë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

**í›„ë³´ì ëª…ë‹¨**: {', '.join(self.candidates)}

**ê¸°ì‚¬ ì œëª©**: {article['title']}

**ê¸°ì‚¬ ë‚´ìš©**: {article['content']}

**ì¶œë ¥ í˜•ì‹ (JSON)**:
{{
    "relationships": [
        {{
            "person1": "í›„ë³´ìëª…",
            "person2": "í›„ë³´ìëª… ë˜ëŠ” ê´€ë ¨ ì¸ë¬¼",
            "relation_type": "ì •ì¹˜ì ë™ë§¹|ê²½ìŸ|í•™ì—°|ì§€ì—°|ì‚¬ì œ|í˜‘ë ¥|ë¹„íŒ|ì§€ì§€|ì¤‘ë¦½",
            "strength": 0.0-1.0,
            "direction": "ì–‘ë°©í–¥|person1â†’person2|person2â†’person1",
            "evidence": "ê´€ê³„ë¥¼ ë³´ì—¬ì£¼ëŠ” ê¸°ì‚¬ ì† í•µì‹¬ ë¬¸ì¥",
            "sentiment": "ê¸ì •|ë¶€ì •|ì¤‘ë¦½"
        }}
    ]
}}

**ê·œì¹™**:
1. í›„ë³´ì ëª…ë‹¨ì— ìˆëŠ” ì¸ë¬¼ë§Œ ì¶”ì¶œ
2. ê¸°ì‚¬ì— ëª…ì‹œì ìœ¼ë¡œ ë‚˜íƒ€ë‚œ ê´€ê³„ë§Œ ì¶”ì¶œ
3. ì¶”ì¸¡ì´ë‚˜ ì¶”ë¡  ê¸ˆì§€
4. ê´€ê³„ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
"""
        
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            response = model.generate_content(prompt)
            
            response_text = response.text
            
            # JSON ì¶”ì¶œ
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', 
                                  response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
            
            result = json.loads(response_text)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for rel in result.get('relationships', []):
                rel['source_article'] = article['title']
                rel['url'] = article['url']
                rel['date'] = article['date']
                rel['keyword'] = article['source']
            
            return result.get('relationships', [])
            
        except Exception as e:
            print(f"    Gemini API ì˜¤ë¥˜: {e}")
            return []
    
    def process_articles(self, articles):
        """ê¸°ì‚¬ ëª©ë¡ ì²˜ë¦¬ ë° ê´€ê³„ ì¶”ì¶œ"""
        print(f"\n{'='*60}")
        print(f"Claude APIë¡œ ê´€ê³„ ì¶”ì¶œ ì‹œì‘ ({len(articles)}ê°œ ê¸°ì‚¬)")
        print("="*60 + "\n")
        
        all_relationships = []
        
        for i, article in enumerate(articles, 1):
            if i % 5 == 0:
                print(f"ì§„í–‰: {i}/{len(articles)}")
            
            relationships = self.extract_relationships_with_claude(article)
            all_relationships.extend(relationships)
            
            time.sleep(0.5)  # API ê³¼ë¶€í•˜ ë°©ì§€
        
        return pd.DataFrame(all_relationships)


# ============================================================================
# 3. ì‹¤í–‰
# ============================================================================

def main():
    print("\n" + "="*60)
    print("ì¶©ë¶ ë„ì§€ì‚¬ í›„ë³´ ê´€ê³„ë§ ë¶„ì„")
    print("ì§€ì—­ ì‹ ë¬¸ì‚¬ RSS ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘")
    print(f"ëŒ€ìƒ ì–¸ë¡ ì‚¬: {len(REGIONAL_NEWS_SOURCES)}ê°œ")
    print("="*60)
    
    # API í‚¤ í™•ì¸
    if not os.environ.get("GEMINI_API_KEY"):
        print("\nâš ï¸  GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ëª…ë ¹ í”„ë¡¬í”„íŠ¸ì—ì„œ:")
        print("  set GEMINI_API_KEY=your-api-key")
        return
    
    # ìˆ˜ì§‘ê¸° ìƒì„±
    collector = LocalNewsCollector()
    
    # 1. RSSì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
    articles = collector.collect_all()
    
    if not articles:
        print("\n" + "="*60)
        print("âš ï¸  ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        print("="*60)
        print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print("1. êµ¬ê¸€ ì•Œë¦¼ RSS ì‚¬ìš© (ê°€ì¥ í™•ì‹¤!)")
        print("   - https://www.google.com/alerts")
        print("   - ê° í›„ë³´ìëª…ìœ¼ë¡œ ì•Œë¦¼ ìƒì„±")
        print("   - RSS ì£¼ì†Œ ë³µì‚¬í•´ì„œ ì½”ë“œì— ì…ë ¥")
        print("\n2. RSS ì£¼ì†Œ ì§ì ‘ í™•ì¸:")
        print("   - ì‹ ë¬¸ì‚¬ ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸")
        print("   - í˜ì´ì§€ ì†ŒìŠ¤ ë³´ê¸° (ìš°í´ë¦­)")
        print("   - 'rss' ë˜ëŠ” 'feed' ê²€ìƒ‰")
        print("\n3. í¬ë¡¬ í™•ì¥ í”„ë¡œê·¸ë¨ ì‚¬ìš©:")
        print("   - 'Get RSS Feed URL' ì„¤ì¹˜")
        print("   - ì‹ ë¬¸ì‚¬ ì‚¬ì´íŠ¸ì—ì„œ í´ë¦­")
        return
    
    print(f"\n" + "="*60)
    print(f"âœ… ì´ {len(articles)}ê°œ ê´€ë ¨ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ!")
    print("="*60)
    
    # ì–¸ë¡ ì‚¬ë³„ í†µê³„
    source_counts = {}
    for article in articles:
        source = article['source']
        source_counts[source] = source_counts.get(source, 0) + 1
    
    print("\nì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ìˆ˜:")
    for source, count in sorted(source_counts.items(), 
                                key=lambda x: x[1], reverse=True):
        print(f"  {source}: {count}ê°œ")
    
    # 2. Claude APIë¡œ ê´€ê³„ ì¶”ì¶œ
    df_relationships = collector.process_articles(articles)
    
    if len(df_relationships) == 0:
        print("\nâš ï¸  ì¶”ì¶œëœ ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ê¸°ì‚¬ëŠ” ìˆì§€ë§Œ í›„ë³´ì ê°„ ê´€ê³„ê°€ ëª…í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return
    
    # 3. CSV + Excel ì €ì¥
    df_relationships.to_csv('relationships_raw.csv', 
                           index=False, 
                           encoding='utf-8-sig')
    print(f"\nâœ… CSV ì €ì¥: relationships_raw.csv")
    
    # Excelë„ ì €ì¥ (í•œê¸€ ì•ˆ ê¹¨ì§)
    try:
        import openpyxl
        df_relationships.to_excel('relationships_raw.xlsx', 
                                 index=False, 
                                 engine='openpyxl')
        print(f"âœ… Excel ì €ì¥: relationships_raw.xlsx")
    except:
        print("ğŸ’¡ Excel íŒŒì¼ì„ ë§Œë“¤ë ¤ë©´: pip install openpyxl")
    
    print(f"\nì´ {len(df_relationships)}ê°œ ê´€ê³„ ì¶”ì¶œ")
    
    # 4. ìš”ì•½ í†µê³„
    print("\n" + "="*60)
    print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
    print("="*60)
    
    print(f"\nê´€ê³„ ìœ í˜• ë¶„í¬:")
    rel_counts = df_relationships['relation_type'].value_counts()
    for rel_type, count in rel_counts.items():
        print(f"  {rel_type}: {count}ê°œ")
    
    print(f"\nì–¸ë¡ ì‚¬ë³„ ê´€ê³„ ì¶”ì¶œ:")
    keyword_counts = df_relationships['keyword'].value_counts()
    for keyword, count in keyword_counts.items():
        print(f"  {keyword}: {count}ê°œ")
    
    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("  python network_analysis_premium.R ì‹¤í–‰")
    print("  â†’ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë° ì‹œê°í™”")


if __name__ == "__main__":
    main()
